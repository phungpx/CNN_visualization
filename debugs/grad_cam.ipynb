{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "otherwise-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class gradCAM(nn.Module):\n",
    "    def __init__(self, model, target_module, target_layer, device):\n",
    "        super(gradCAM, self).__init__()\n",
    "        self.target_activations = list()\n",
    "        self.target_gradients = list()\n",
    "        self._register_hook(model, target_module, target_layer)\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _register_hook(self, model, target_module, target_layer):\n",
    "        if target_module not in list(model._modules.keys()):\n",
    "            raise TypeError('target module must be in list of modules of model')\n",
    "        if target_layer not in list(model._modules.get(target_module)._modules.keys()):\n",
    "            raise TypeError('target layer must be in list of layers of module')\n",
    "\n",
    "        def register_forward_hook(module, input, output):\n",
    "            self.target_activations.append(output)\n",
    "\n",
    "        def register_backward_hook(module, grad_input, grad_output):\n",
    "            self.target_gradients.append(grad_output[0])\n",
    "\n",
    "        model._modules[target_module]._modules[target_layer].register_forward_hook(register_forward_hook)\n",
    "        model._modules[target_module]._modules[target_layer].register_backward_hook(register_backward_hook)\n",
    "\n",
    "    def forward(self, sample):  # sample with batch_size = 1\n",
    "        sample = sample.to(self.device)  # [1, C, H, W]\n",
    "        B, C, H, W = sample.shape\n",
    "        preds = self.model(sample)  # [1, num_classes]\n",
    "\n",
    "        categories = torch.argmax(preds, dim=1, keepdims=True)   # [1, num_classes]\n",
    "        batch_onehot = torch.zeros(size=preds.shape, dtype=torch.float, device=self.device)  # [1, num_classes]\n",
    "        batch_onehot.scatter_(dim=1, index=categories, value=1)  # [1, num_classes]\n",
    "        batch_onehot = batch_onehot.requires_grad_(requires_grad=True)  # [1, num_classes]\n",
    "\n",
    "        categories_score = torch.sum(batch_onehot * preds)  # scalar\n",
    "        self.model.zero_grad()\n",
    "        categories_score.backward(retain_graph=True)\n",
    "\n",
    "        target_gradient = self.target_gradients[0]   # [1, Cf, Hf, Wf]\n",
    "        target_activation = self.target_activations[0]   # [1, Cf, Hf, Wf]\n",
    "\n",
    "        weights = torch.mean(target_gradient, dim=(0, 2, 3), keepdims=True)  # [1, Cf, 1, 1]\n",
    "        saliency_map = torch.sum(target_activation * weights, dim=(0, 1), keepdims=True)  # [1, 1, Hf, Wf]\n",
    "        saliency_map = nn.functional.relu(saliency_map)  # [1, 1, Hf, Wf]\n",
    "        saliency_map = nn.functional.interpolate(input=saliency_map, size=(H, W), mode='bilinear', align_corners=False)  # [1, 1, H, W]\n",
    "        saliency_map = saliency_map.squeeze(dim=0).squeeze(dim=0)  # [H, W]\n",
    "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "        saliency_map = saliency_map.cpu().data.numpy()\n",
    "\n",
    "        return saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attractive-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False):\n",
    "        super(Resnet18, self).__init__()\n",
    "        resnet18_model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        self.resnet18_conv = nn.Sequential(*list(resnet18_model.children())[:-2])\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.linear = nn.Linear(in_features=resnet18_model.fc.in_features, out_features=num_classes, bias=True)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.resnet18_conv(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banner-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing definition\n",
    "\n",
    "def preprocess(image_path, image_size=(224, 224), device='cpu'):\n",
    "    image = cv2.imread(image_path)\n",
    "    sample = cv2.resize(image, dsize=image_size)\n",
    "    sample = torch.from_numpy(sample).to(device).to(torch.float)\n",
    "    sample = sample.unsqueeze(dim=0)\n",
    "    sample = sample.permute(0, 3, 1, 2).contiguous()\n",
    "    sample = (sample - sample.mean(dim=(1, 2, 3), keepdim=True)) / sample.std(dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "    return image, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabulous-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization model\n",
    "\n",
    "weight_path = '../../../../VTCC/phungpx/id_info_extraction/models/weights/hole_detection/2103310956/best_model_66_loss=-0.1619.pt'\n",
    "num_classes = 2\n",
    "device = 'cpu'\n",
    "model = Resnet18(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(f=weight_path, map_location='cpu'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occupied-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def preprocess(image_path, image_size=(224, 224), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    image = cv2.imread(image_path)\n",
    "    sample = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    sample = Image.fromarray(sample)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size=image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)])\n",
    "    sample = transform(sample).unsqueeze(dim=0)\n",
    "    return image, sample\n",
    "\n",
    "\n",
    "image_path = '/home/phungpx/Downloads/test/bird.png'\n",
    "image, sample = preprocess(image_path)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "forty-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "target_module = 'layer4'\n",
    "target_layer = '2'\n",
    "visualizer = gradCAM(model=model, target_module=target_module, target_layer=target_layer, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "polyphonic-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 2000)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cam = visualizer(sample)\n",
    "cam = cv2.resize(cam, dsize=(image.shape[1], image.shape[0]))\n",
    "print(cam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "accompanied-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(image, cam):\n",
    "    image = np.float32(image) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    heatmap = heatmap + np.float32(image)\n",
    "    heatmap = heatmap / np.max(heatmap)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "accepting-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = show_cam_on_image(image, cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "hindu-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('class activation map', cam)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "empty-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = torch.argmax(preds, dim=1, keepdims=True)\n",
    "batch_onehot = torch.zeros(size=preds.shape, dtype=torch.float, device='cpu')\n",
    "batch_onehot.scatter_(dim=1, index=categories, value=1)\n",
    "batch_onehot = batch_onehot.requires_grad_(requires_grad=True)\n",
    "score = torch.sum(batch_onehot * preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "significant-doctrine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck(\n",
      "  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "\n",
      "grad_input:  <class 'tuple'>\n",
      "grad_input[0]:  <class 'torch.Tensor'>\n",
      "grad_output:  <class 'tuple'>\n",
      "grad_output[0]:  <class 'torch.Tensor'>\n",
      "\n",
      "grad_input size: torch.Size([1, 2048, 7, 7])\n",
      "grad_output size: torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "score.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-southwest",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "private-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /media/phungpx/WORKSPACE/PHUNGPX/CAMs/debugs\n",
      "changed dir: /media/phungpx/WORKSPACE/PHUNGPX/CAMs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'current dir: {os.getcwd()}')\n",
    "os.chdir('/media/phungpx/WORKSPACE/PHUNGPX/CAMs/')\n",
    "print(f'changed dir: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secret-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import utils\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-brain",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "economic-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_yaml('./modules/gradCAM/config.yaml')\n",
    "visualizer = utils.create_instance(config['gradCAM_pretrained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egyptian-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/phungpx/Downloads/test/bird.png'\n",
    "image = cv2.imread(image_path)\n",
    "grad_cam, heatmap = visualizer(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "homeless-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(image, max_dim=800):\n",
    "    h, w = image.shape[:2]\n",
    "    if (h > w) and (h > max_dim):\n",
    "        image = cv2.resize(image, dsize=(int(max_dim * w / h), max_dim))\n",
    "    elif (w > h) and (w > max_dim):\n",
    "        image = cv2.resize(image, dsize=(max_dim, int(max_dim * h / w)))\n",
    "    return image\n",
    "\n",
    "cv2.imshow('grad CAM', _resize(heatmap))\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-benjamin",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secret-festival",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7b43ba605678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResnet18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modules:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layers in resnet18_conv:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resnet18_conv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..models.definitions.resnet18 import Resnet18\n",
    "model = Resnet18(num_classes=2)\n",
    "print('modules:', model._modules.keys())\n",
    "print('layers in resnet18_conv:', model._modules['resnet18_conv']._modules.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thousand-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_yaml('./modules/gradCAM/config.yaml')\n",
    "visualizer = utils.create_instance(config['gradCAM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "def _resize(image, max_dim=800):\n",
    "    h, w = image.shape[:2]\n",
    "    if (h > w) and (h > max_dim):\n",
    "        image = cv2.resize(image, dsize=(int(max_dim * w / h), max_dim))\n",
    "    elif (w > h) and (w > max_dim):\n",
    "        image = cv2.resize(image, dsize=(max_dim, int(max_dim * h / w)))\n",
    "    return image\n",
    "\n",
    "image_dir = Path('../../ID_CARD/hole_classification/dataset/test/hole/CMQD_A/')\n",
    "image_paths = list(image_dir.glob('**/*.*'))\n",
    "print(len(image_paths))\n",
    "for i, image_path in enumerate(image_paths[4:]):\n",
    "    if i == 10:\n",
    "        break\n",
    "    image = cv2.imread(str(image_path))\n",
    "    gradCAM, heatmap = visualizer(image)\n",
    "    cv2.imshow('grad CAM', _resize(heatmap))\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-cisco",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
